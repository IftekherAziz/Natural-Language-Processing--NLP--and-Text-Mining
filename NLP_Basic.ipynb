{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61632555-2ad6-4709-af04-a3d6724da30a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:228: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "# Load all items from NLTK book module\n",
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a82e642c-47d4-4ace-ae60-11bb2f9f1e3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adventure',\n",
       " 'belles_lettres',\n",
       " 'editorial',\n",
       " 'fiction',\n",
       " 'government',\n",
       " 'hobbies',\n",
       " 'humor',\n",
       " 'learned',\n",
       " 'lore',\n",
       " 'mystery',\n",
       " 'news',\n",
       " 'religion',\n",
       " 'reviews',\n",
       " 'romance',\n",
       " 'science_fiction']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exploring brown corpus\n",
    "from nltk.corpus import brown\n",
    "brown.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "201fdffa-533a-49c1-b49c-f7e429fa4e76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['It', 'is', 'not', 'news', 'that', 'Nathan', ...]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extracting a list of words from the review category of the Brown Corpus \n",
    "review_words = brown.words(categories='reviews')\n",
    "review_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "715add85-1d61-4492-b5b1-be4cbd49f811",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40704"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the length of the list\n",
    "len(review_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206c80f5-81ea-4f6b-ab04-6dcbea77dbc2",
   "metadata": {},
   "source": [
    "# Text Extration and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ca885c-038f-4781-9dde-1d004c3554d3",
   "metadata": {},
   "source": [
    "### Tokenization: \n",
    "It the process of removing sensative data and placing unique symbols of identification in it's place to retain all the essential information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceea4737-5e51-45a3-84b2-d52f58662440",
   "metadata": {},
   "source": [
    "### N-grams:\n",
    "N-grams are a fundamental concept in natural language processing and computational linguistics. An n-gram is a contiguous sequence of n items from a given sample of text or speech. The items can be phonemes, syllables, letters, words, or base pairs according to the application. \n",
    "\n",
    "N-grams are used in various applications including text mining, spell checking, speech recognition, and statistical machine learning models for language."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3d5d31-5854-4ff1-97d8-46341365e1da",
   "metadata": {},
   "source": [
    "### Here's a breakdown of how n-grams work:\n",
    "\n",
    "- Unigrams (1-grams): These are single units (like words or characters) in the text. \n",
    "##### For example, in the sentence \"The cat sat on the mat,\" unigrams would include 'The', 'cat', 'sat', 'on', 'the', 'mat'.\n",
    "\n",
    "- Bigrams (2-grams): These are sequences of two contiguous items. \n",
    "##### In the same sentence, bigrams would include 'The cat', 'cat sat', 'sat on', 'on the', 'the mat'.\n",
    "\n",
    "- Trigrams (3-grams): These are sequences of three contiguous items. \n",
    "##### In our example, trigrams would be 'The cat sat', 'cat sat on', 'sat on the', 'on the mat'.\n",
    "\n",
    "- Higher-order n-grams: Similarly, you can have 4-grams, 5-grams, etc., which involve longer sequences of words or characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1e49563-a8ff-425d-9482-e188d3e3ed20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'quick'),\n",
       " ('quick', 'brown'),\n",
       " ('brown', 'fox'),\n",
       " ('fox', 'jumps'),\n",
       " ('jumps', 'over'),\n",
       " ('over', 'the'),\n",
       " ('the', 'lazy'),\n",
       " ('lazy', 'dog')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# Example text\n",
    "text = \"The quick brown fox jumps over the lazy dog\"\n",
    "\n",
    "# Tokenize the text into words\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Generate bigrams,\n",
    "bigrams = list(ngrams(tokens, 2))\n",
    "bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62a151e9-caa2-4da6-b625-a814f9410025",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'quick', 'brown'),\n",
       " ('quick', 'brown', 'fox'),\n",
       " ('brown', 'fox', 'jumps'),\n",
       " ('fox', 'jumps', 'over'),\n",
       " ('jumps', 'over', 'the'),\n",
       " ('over', 'the', 'lazy'),\n",
       " ('the', 'lazy', 'dog')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate bigrams, trigrams, and 4-grams\n",
    "trigrams = list(ngrams(tokens, 3))\n",
    "trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5ee3ca0-8826-4efb-95e0-6ab5c46f4a7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'quick', 'brown', 'fox'),\n",
       " ('quick', 'brown', 'fox', 'jumps'),\n",
       " ('brown', 'fox', 'jumps', 'over'),\n",
       " ('fox', 'jumps', 'over', 'the'),\n",
       " ('jumps', 'over', 'the', 'lazy'),\n",
       " ('over', 'the', 'lazy', 'dog')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fourgrams = list(ngrams(tokens, 4))\n",
    "fourgrams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45e5a7d-d3ff-4440-837d-154e05e854eb",
   "metadata": {},
   "source": [
    "### Stop word removal: \n",
    "It is a common preprocessing step in natural language processing (NLP) and text mining. \"Stop words\" are words that are filtered out before or after processing text. \n",
    "\n",
    "They are usually common words in a language (such as \"the\", \"is\", \"in\", \"on\", etc.) that do not add much meaning to a sentence and are thus omitted from the analysis to reduce noise and computational cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e170d1cd-7c4c-43e1-bed7-f613c93f9b9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Set of English stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3267931f-2592-4b62-b596-753b00877bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['example', 'showing', 'stop', 'word', 'filtration', '.']\n"
     ]
    }
   ],
   "source": [
    "# After stop word removal we can see:\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Example sentence\n",
    "text = \"This is an example showing off stop word filtration.\"\n",
    "\n",
    "# Tokenize the text\n",
    "word_tokens = word_tokenize(text)\n",
    "\n",
    "# Filter out the stop words\n",
    "filtered_sentence = [word for word in word_tokens if not word.lower() in stop_words]\n",
    "\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415d9e12-6116-47c0-a5af-ca0c9489c3c1",
   "metadata": {},
   "source": [
    "### Stemming: \n",
    "\n",
    "It is a process in natural language processing and text mining that involves reducing words to their word stem, base or root form. \n",
    "\n",
    "Stemming is often used in search engines, text mining, and information retrieval to improve query matching. \n",
    "\n",
    "By reducing a word to its stem, different forms of the same word (like \"running\", \"ran\", \"runs\") are treated as the same, which can simplify text processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dde77da-e1fb-4a07-bdca-139f85a6f2aa",
   "metadata": {},
   "source": [
    "### Popular Stemming Algorithms:\n",
    "- Porter Stemmer: One of the most widely used and oldest stemmers, developed by Martin Porter in 1980. It's known for its simplicity and speed.\n",
    "\n",
    "- Lancaster Stemmer: Developed at Lancaster University, it is more aggressive than the Porter stemmer. It iterates over the word more times and has more reduction rules.\n",
    "\n",
    "- Snowball Stemmer: Also known as the \"Porter2\" stemmer, it's a slightly improved version of the Porter stemmer and part of a framework of stemming algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a6015a4-96eb-4377-a30f-8ede3701392a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'runner', 'like', 'run', 'and', 'run', 'fast']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Example text\n",
    "text = \"The runner likes running and runs fast\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Create a stemmer\n",
    "porter = PorterStemmer()\n",
    "\n",
    "# Stem each word in the tokenized text\n",
    "stemmed_words = [porter.stem(word) for word in tokens]\n",
    "\n",
    "print(stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec1ebe0-7a6f-42c3-bcd1-b82f48debcc7",
   "metadata": {},
   "source": [
    "### Lemmatization: \n",
    "\n",
    "This is the method of grouping the various inflected of word so that they can be analyzed as one item. It uses a vocabulary list and morphological analysis ( POS of a word) to get the root word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1debc0a6-a173-413e-b16a-e7485d378550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "foot\n",
      "cactus\n",
      "goose\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(lemmatizer.lemmatize('feet'))\n",
    "print(lemmatizer.lemmatize('cacti'))\n",
    "print(lemmatizer.lemmatize('geese'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42d5a9c9-a596-40b9-b1f1-e404d05b88bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loving\n",
      "love\n"
     ]
    }
   ],
   "source": [
    "# without a POS tag, lemmatizer assumes everything as a noun\n",
    "print(lemmatizer.lemmatize('loving'))\n",
    "print(lemmatizer.lemmatize('loving', 'v')) #with POS tag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be4590c-5259-48a3-9dcc-d89102fb2a2f",
   "metadata": {},
   "source": [
    "### Part of Speech(POS) Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cf2f32-6a6e-47cf-8cc5-dc0769ed3723",
   "metadata": {},
   "source": [
    "Part-of-Speech (POS) tagging is an important process in natural language processing (NLP) where each word in a sentence is assigned a part-of-speech tag, such as noun, verb, adjective, adverb, etc. POS tagging is a fundamental step in text analysis and helps in understanding the grammar and structure of sentences, which is crucial for various NLP tasks like parsing, named entity recognition, question answering, and machine translation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afccc83-73b9-4046-80aa-cd4cdb03a603",
   "metadata": {},
   "source": [
    "### Types of POS Tags:\n",
    "\n",
    "- Noun (NN): Names of things, places, people, etc.\n",
    "- Verb (VB): Actions or states of being.\n",
    "- Adjective (JJ): Describes or modifies a noun.\n",
    "- Adverb (RB): Modifies a verb, an adjective, or another adverb.\n",
    "- Pronoun (PRP): Stands in for a noun or noun phrase.\n",
    "- Preposition (IN): Shows relationships between nouns (or pronouns) and other words.\n",
    "- Article (DT): The articles of a sentence\n",
    "- VBZ: This tag denotes a verb that is in the third person singular present tense."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc3292f-e118-4cd3-b90b-5b54b823d635",
   "metadata": {},
   "source": [
    "### Applications : POS tagging is widely used in:\n",
    "\n",
    "- Syntax Parsing: Understanding the grammatical structure of sentences.\n",
    "- Word Sense Disambiguation: Determining the meaning of a word based on context.\n",
    "- Information Retrieval: Enhancing search algorithms.\n",
    "- Machine Translation: Translating text from one language to another."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eceeddf2-2168-4b5b-9185-b1c1ecd15b82",
   "metadata": {},
   "source": [
    "### Challenges:\n",
    "- Ambiguity: A word may have multiple POS tags depending on the context (e.g., \"book\" can be a noun or a verb).\n",
    "- Language Complexity: Different languages have different grammatical rules and structures, making POS tagging more challenging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "00ceb9cb-ab66-4595-8747-fc879a219074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "# Example text\n",
    "text = \"The quick brown fox jumps over the lazy dog\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# POS Tagging\n",
    "tagged = nltk.pos_tag(tokens)\n",
    "\n",
    "print(tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c3e36e-9d83-4170-a967-5287a11733e1",
   "metadata": {},
   "source": [
    "## Name Entity Recognition ( NER )\n",
    "\n",
    "Named Entity Recognition (NER) is a subtask of information extraction in natural language processing (NLP) that involves identifying and classifying named entities in text into predefined categories. Named entities are real-world objects that have a name, such as people, organizations, locations, dates, products, etc. The purpose of NER is to assign these entities to categories such as PERSON, ORGANIZATION, LOCATION, DATE, TIME, PRODUCT, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cfa6cb-d491-4cdb-8d49-0c30c1b00f91",
   "metadata": {},
   "source": [
    "### Techniques Used in NER\n",
    "- Rule-Based Approaches: Use hand-crafted linguistic rules. For example, a rule might say that anything that looks like a proper name and is preceded by a title (like Mr., Mrs., Dr., etc.) should be classified as a PERSON.\n",
    "\n",
    "- Statistical Approaches: Use algorithms like Hidden Markov Models (HMMs), Support Vector Machines (SVMs), or Conditional Random Fields (CRFs). These models are trained on large datasets of annotated text.\n",
    "\n",
    "- Deep Learning Approaches: Employ neural network models, which can learn complex patterns from large amounts of data. Models like LSTM (Long Short-Term Memory), BiLSTM (Bidirectional LSTM), or BERT (Bidirectional Encoder Representations from Transformers) are popular."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56abf7f5-0a3c-40e9-a269-0c1dbb42f383",
   "metadata": {},
   "source": [
    "### Applications of NER\n",
    "- Information Retrieval: Improves the search for specific information in large datasets.\n",
    "- Content Classification: Helps in categorizing content and understanding the main topics in text.\n",
    "- Customer Support: Automates the extraction of relevant information from customer inquiries.\n",
    "- Sentiment Analysis: Identifies entities in text to understand sentiments towards specific products, services, or brands.\n",
    "- Machine Translation: Improves the accuracy of translation by understanding the role of named entities in sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f809c15c-1884-47a9-b78c-610711bc9274",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sent_tokenize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 11\u001b[0m\n\u001b[0;32m      5\u001b[0m doc \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mGoogle is an American multinational technology company that specializes in\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124minternet-related services and products, which include online advertising technologies,\u001b[39m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124msearch engine, cloud computing, and hardware. It was founded in 1998 by Larry Page and Sergey Brin\u001b[39m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124mwhile they were Ph.D. students at Stanford University in California.\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Tokenize the doc into sentences\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m tokenized_doc \u001b[38;5;241m=\u001b[39m \u001b[43msent_tokenize\u001b[49m(doc)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Tokenize each sentence into words and then POS-tagging\u001b[39;00m\n\u001b[0;32m     14\u001b[0m tagged_sentences \u001b[38;5;241m=\u001b[39m [pos_tag(word_tokenize(sentence)) \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m tokenized_doc]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sent_tokenize' is not defined"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries from NLTK\n",
    "import nltk\n",
    "\n",
    "# Sample text for Named Entity Recognition\n",
    "doc = \"\"\"Google is an American multinational technology company that specializes in\n",
    "internet-related services and products, which include online advertising technologies,\n",
    "search engine, cloud computing, and hardware. It was founded in 1998 by Larry Page and Sergey Brin\n",
    "while they were Ph.D. students at Stanford University in California.\"\"\"\n",
    "\n",
    "# Tokenize the doc into sentences\n",
    "tokenized_doc = sent_tokenize(doc)\n",
    "\n",
    "# Tokenize each sentence into words and then POS-tagging\n",
    "tagged_sentences = [pos_tag(word_tokenize(sentence)) for sentence in tokenized_doc]\n",
    "\n",
    "# Chunk sentences to NE (Named Entity) chunks\n",
    "ne_chunked_sents = [ne_chunk(tagged) for tagged in tagged_sentences]\n",
    "\n",
    "# Extract all named entities\n",
    "named_entities = []\n",
    "\n",
    "for ne_tagged_sentence in ne_chunked_sents:\n",
    "    for tagged_tree in ne_tagged_sentence:\n",
    "        # Extracting NEs here\n",
    "        if hasattr(tagged_tree, 'label'):\n",
    "            entity_name = ' '.join(c[0] for c in tagged_tree.leaves())  # Get the NE name\n",
    "            entity_type = tagged_tree.label()  # Get the NE label\n",
    "            named_entities.append((entity_name, entity_type))\n",
    "\n",
    "named_entities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd631e92-b7f6-4f6c-a778-2e791d8498bf",
   "metadata": {},
   "source": [
    "### Working with Brown Corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62354ede-5163-4092-9cd7-aa60371fa06e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
